{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #disables GPU \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#tf.__version__\n",
    "from tensorflow.python.client import device_lib\n",
    "#print(device_lib.list_local_devices())\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10,10]\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "#https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/\n",
    "\n",
    "def load_img(img_path,gray = False):\n",
    "    \n",
    "    if gray:\n",
    "        return cv2.imread(img_path , cv2.IMREAD_GRAYSCALE) \n",
    "    \n",
    "    im = cv2.imread(img_path ) \n",
    "    im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import decode_predictions\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet',\n",
    "                  include_top=True,\n",
    "                  input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filters(layer_name):\n",
    "    layer = [l for l in model.layers if layer_name == l.name][0]\n",
    "\n",
    "    filters, biases = model.layers[1].get_weights()\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    n_filters, ix = 6, 1\n",
    "    for i in range(n_filters):\n",
    "        # get the filter\n",
    "        f = filters[:, :, :, i]\n",
    "        # plot each channel separately\n",
    "        for j in range(3):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = plt.subplot(n_filters, 3, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            plt.imshow(f[:, :, j], cmap='gray')\n",
    "            ix += 1\n",
    "    # show the figure\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that does the above step but for any all layers in a block\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "def cnn_lap(image, layer_name):\n",
    "    mag = []\n",
    "    layer = [l for l in model.layers if layer_name == l.name][0]\n",
    "\n",
    "    layer = [l for l in model.layers if layer_name == l.name][0]\n",
    "\n",
    "    filters, biases = model.layers[1].get_weights()\n",
    "    f_min, f_max = filters.min(), filters.max()\n",
    "    filters = (filters - f_min) / (f_max - f_min)\n",
    "    filters = tf.constant( filters[:, :, :, 1], dtype=tf.float32)\n",
    "    image = np.array([image])\n",
    "    image =  tf.constant(image, dtype=tf.float32)\n",
    "    filtered_img = tf.nn.conv2d(np.array([image]), filters = np.array([filters]), strides=[1, 1, 1, 1], padding='VALID').numpy()\n",
    "    print(\"filtered img \", type(filtered_img))\n",
    "    laps = []\n",
    "    for i in range(0, 64):\n",
    "        print(i)\n",
    "        out = filtered_img[i][0]\n",
    "        plt.imshow(out)\n",
    "\n",
    "        plt.show()\n",
    "        dst = cv2.Laplacian(out, cv2.CV_32F )\n",
    "        laps.append(dst)\n",
    "\n",
    "    mag.append(np.linalg.norm(np.abs(laps), axis = 0))\n",
    "    #mag.append(np.sum(laps, axis = 0))\n",
    "    #scaled_mag = min_max_scaler.fit_transform(mag[0])\n",
    "\n",
    "    #mag /= mag.max()/255.0'''\n",
    "    return mag[0] * 1.0/mag[0].max()  , filtered_img, np.array(laps)\n",
    "    #return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_arg_exrtrema_2(mat1, mat2):\n",
    "\n",
    "    #Use a moving window to find local max/min in section. Determine coordinate of max pixel in image.\n",
    "    idx = []\n",
    "    thresh = np.abs(np.amax(mat2) - np.abs(np.amin(mat2)))\n",
    "    #print(\"Thresh = \", thresh)\n",
    "\n",
    "    r = cv2.cornerHarris(mat2,2,3,0.001)\n",
    "    r = cv2.dilate(r,None)\n",
    "    for i in range(1, mat1.shape[0]-1):\n",
    "        for j in range(1, mat1.shape[1]-1):\n",
    "            if r[i,j]  > 0.01*np.amax(r) : #if pixel is not on edge\n",
    "                pixel_of_interest = mat2[i,j]\n",
    "                #if np.abs(pixel_of_interest)>0.15:\n",
    "\n",
    "                neighbours = mat2[i-1:i+2, j-1:j+2]\n",
    "                neighbours[1,1] = np.NaN\n",
    "                neighbours_below = mat1[i-1:i+2, j-1:j+2]\n",
    "                if (pixel_of_interest > np.nanmax(neighbours) and pixel_of_interest > np.nanmax(neighbours_below)) or (pixel_of_interest < np.nanmin(neighbours) and pixel_of_interest < np.nanmin(neighbours_below)):\n",
    "                    idx.append(np.array([j,i]))\n",
    "\n",
    "    return np.unique(idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescriptors(layer_output, coords):\n",
    "    #fig2 = plt.figure(figsize=(30, 30))\n",
    "\n",
    "    layers = []\n",
    "    '''for i in range(len(layer_output)):\n",
    "        layer_output[i] = cv2.resize(layer_output[i], (224, 224), interpolation = cv2.INTER_LINEAR )'''\n",
    "    layer_output = np.stack(layer_output)\n",
    "    '''for i in range(0,64):\n",
    "        layers.append(layer_output[:,:,i])'''\n",
    "    #layers = np.stack(layers)\n",
    "\n",
    "    descriptors = []\n",
    "    for col, row in  coords:\n",
    "        #d_vec = np.array(layer_output[:,row, col ]).flatten()\n",
    "        d_vec = np.array(layer_output[:,row-1:row+2, col-1:col+2 ]).flatten()\n",
    "        d_vec = np.abs(d_vec)\n",
    "        d_vec *= 1.0/d_vec.max()\n",
    "        descriptors.append(d_vec)\n",
    "    return descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_and_descriptors(img):\n",
    "    keypoint_coords = []\n",
    "\n",
    "    #mag_1,_,_ = cnn_lap(img, layer_name=model.layers[0].name)\n",
    "    mag_2 ,layer_out,lap_out = cnn_lap(img, layer_name=model.layers[1].name)\n",
    "    mag_3,out,_ = cnn_lap(img, layer_name=model.layers[2].name)\n",
    "    #_,layer_out,_ = cnn_lap(img, layer_name=model.layers[7].name)\n",
    "    \n",
    "    keypoint_coords = np.unique(local_arg_exrtrema_2(mag_1, mag_2 ), axis=0)\n",
    "\n",
    "    desc = getDescriptors(lap_out, keypoint_coords)\n",
    "\n",
    "    k=[]\n",
    "    for row, col in keypoint_coords:\n",
    "        #print(float(row), float(col))\n",
    "        keypoint = cv2.KeyPoint()\n",
    "        keypoint.pt = (float(row), float(col))\n",
    "        keypoint.octave = 0\n",
    "        keypoint.size = 0\n",
    "        keypoint.response = 0\n",
    "        k.append(keypoint)\n",
    "\n",
    "\n",
    "    return np.array(k), np.array(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Startin\n",
      "(4032, 3024, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered img  <class 'numpy.ndarray'>\n",
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 460.8x345.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9556/390224804.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mkp1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdes1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keypoints_and_descriptors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mkp2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdes2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keypoints_and_descriptors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Got kp1 and desc1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdes1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9556/140295502.py\u001b[0m in \u001b[0;36mget_keypoints_and_descriptors\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#mag_1,_,_ = cnn_lap(img, layer_name=model.layers[0].name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmag_2\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlayer_out\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlap_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_lap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmag_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_lap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#_,layer_out,_ = cnn_lap(img, layer_name=model.layers[7].name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9556/1316506174.py\u001b[0m in \u001b[0;36mcnn_lap\u001b[1;34m(image, layer_name)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [6.4, 4.8]\n",
    "\n",
    "data_path1='./SYS809_projet2021_sequences1/new_livre1.JPG'\n",
    "data_path2='./SYS809_projet2021_sequences1/livre2.JPG'\n",
    "\n",
    "print(\"Startin\")\n",
    "img_shape=[224,224]\n",
    "img1 = i = cv2.imread(data_path1)\n",
    "img2 = cv2.imread(data_path2)\n",
    "print(img1.shape)\n",
    "#plt.imshow(img1[0])\n",
    "\n",
    "\n",
    "kp1,des1 = get_keypoints_and_descriptors(img1)\n",
    "kp2,des2 = get_keypoints_and_descriptors(img2)\n",
    "print(\"Got kp1 and desc1\", kp1.shape, des1.shape)\n",
    "print(\"Got kp2 and desc2\", len(kp2), len(des2))\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params,search_params)\n",
    "\n",
    "#knnMatch retourne les 2 plus proche voisin pour chaque membre de des1\n",
    "matches = flann.knnMatch(des1,des2,k=2)\n",
    "#Les objects DMatch contiennent index de du point-cle de ds1 match dans le champ queryIdx\n",
    "#et l'index du point-cle appartenant a des2 dans le champ trainIdx\n",
    "print(\"Done Match\")\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "print(\"len match:\", len(matches))\n",
    "\n",
    "# ratio test as per Lowe's paper,\n",
    "good = []\n",
    "for i,(m,n) in enumerate(matches):\n",
    "    if m.distance < .7 * n.distance:\n",
    "        good.append(m)\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = -1, #(0,255,0),\n",
    "                   singlePointColor = (0,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags = 2|4)\n",
    "\n",
    "img1 = i = cv2.imread(data_path1)\n",
    "img2 = cv2.imread(data_path2)\n",
    "img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,matches,None, **draw_params )\n",
    "\n",
    "print(\"len match:\", len(matches))\n",
    "print(\"len good:\", len(good))\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(img3),plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img1_pts = np.float32([ kp.pt for kp in kp1 ])\n",
    "img2_pts = np.float32([ kp.pt for kp in kp2])\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img1)\n",
    "plt.scatter(list(list(zip(*img1_pts))[0]), list(list(zip(*img1_pts))[1]), s=[1], c='r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img2)\n",
    "plt.scatter(list(list(zip(*img2_pts))[0]), list(list(zip(*img2_pts))[1]), s=[1], c='r')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "475d1117ccb240fef505c545958bb6f28a60dd9db01ade801a106393bc57993a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
